{"title":"爬虫之批量下载Admin5源码","date":"2018-08-01T01:45:40.000Z","slug":"crawl-admin5","tags":["Python","爬虫"],"updated":"2018-08-01T01:45:06.140Z","content":"<h2 id=\"项目介绍\"><a href=\"#项目介绍\" class=\"headerlink\" title=\"项目介绍\"></a>项目介绍</h2><p><a href=\"http://down.admin5.com\" target=\"_blank\" rel=\"noopener\"><code>Admin5</code></a> 提供了各种语言的开源项目，包括<code>asp</code> <code>php</code> <code>.net</code> <code>jsp</code>等。最近开始玩代码审计，到处找项目很麻烦，所以打算写一个爬虫，借助Admin5批量下载各类开源项目。</p>\n<h2 id=\"分析页面\"><a href=\"#分析页面\" class=\"headerlink\" title=\"分析页面\"></a>分析页面</h2><p>首先，分析下<code>Admin5</code>的页面。</p>\n<div class=\"article-img\"><p><img src=\"/post/crawl-admin5/screenshot-1.png\" alt=\"截图-1\"></p></div>\n<p>可以看到，页面数据直接在html中显示的，并不是JS渲染上去的，这就比较好爬了。在响应的请求中，页面用的是<code>gb2312</code>编码，打印的时候需要注意一下，不然会乱码。建议设置编码为<code>gbk</code>，因为<code>gb2312</code>的支持的字没有<code>gbk</code>多，像打印<code>囧</code>的时候，就会抛异常。</p>\n<p>首页的地址是<code>http://down.admin5.com/php/</code>，往后翻页，再看看首页的地址。</p>\n<div class=\"article-img\"><p><img src=\"/post/crawl-admin5/screenshot-2.png\" alt=\"截图-2\"></p></div>\n<p>变成了<code>http://down.admin5.com/php/list_30_1.html</code>。</p>\n<p>那么就可以得出页面的地址规则 <code>php/list_30_页码.html</code>。</p>\n<div class=\"article-img\"><p><img src=\"/post/crawl-admin5/screenshot-3.png\" alt=\"截图-3\"></p></div>\n<p>这里使用<code>XPATH表达式</code>可以很方便的获取页面数据，<code>//div[@class=&quot;lists_bigimg_right&quot;]/a</code>，获取当前页所有项目详情的 <code>a</code> 标签，即详情页的地址。</p>\n<p>随便进入一个项目的详情，找到下载地址。</p>\n<div class=\"article-img\"><p><img src=\"/post/crawl-admin5/screenshot-4.png\" alt=\"截图-4\"></p></div>\n<p>使用 XPATH : <code>//ul[@class=&quot;down-anniu&quot;]//li/a/@href</code>，获取下载地址，这条XPATH是获取所有的下载点地址，实际用不到这么多，可以修改下 <code>//ul[@class=&quot;down-anniu&quot;]/li[1]/a/@href</code>，只获取1条。</p>\n<p>当然，这个页面还包含了其它的数据，同样可以用 XPATH 采集，就不详细写了，后面在代码中会有。</p>\n<h2 id=\"运行流程\"><a href=\"#运行流程\" class=\"headerlink\" title=\"运行流程\"></a>运行流程</h2><p>下面梳理下我们爬虫的运行流程。</p>\n<ol>\n<li><p>访问<code>http://down.admin5.com/php/</code>，<code>asp</code>,<code>.net</code>等语言项目的页面其实是一样的，改下路径就行，这里以<code>php</code>的为例。</p>\n</li>\n<li><p>获取后续页面的地址，即下一页。由于<code>Admin5</code>的页面比较简单，可以直接获取总页数，然后用<code>for</code>循环，遍历<code>http://down.admin5.com/php/list_30_页码.html</code>。</p>\n</li>\n<li><p>获取每个项目的详细页地址，并跟进。</p>\n</li>\n<li><p>提取项目的详细参数，获取下载地址，加入下载队列。</p>\n</li>\n<li><p>下载模块不断取出队列的任务，下载项目，直到队列为空。</p>\n</li>\n</ol>\n<h2 id=\"具体实现\"><a href=\"#具体实现\" class=\"headerlink\" title=\"具体实现\"></a>具体实现</h2><p>这个项目比较简单，所以没有用<code>scrapy</code>等很重的框架去做。就用最简单的<code>requests</code>库去实现，下面设计一下具体的业务模型。</p>\n<h3 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h3><ol>\n<li>ProjectItem<ul>\n<li>name</li>\n<li>size</li>\n<li>update_date</li>\n<li>official_url</li>\n<li>download_url</li>\n</ul>\n</li>\n</ol>\n<p>这个模型包含每个项目的属性，包括项目名称，大小，更新日期，官方地址，下载地址。</p>\n<ol start=\"2\">\n<li>Downloader<ul>\n<li>download_queue</li>\n<li>cache_path</li>\n<li>save_path</li>\n<li>add_task(projcet_item)</li>\n<li>print_progress()</li>\n<li>save_cache()</li>\n</ul>\n</li>\n</ol>\n<p>下载器，这个模型负责下载项目，维护下载队列，监控下载进度，并对下载过的项目地址做缓存，防止重复下载。</p>\n<h4 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h4><p>已上传到 <a href=\"https://github.com/CHN-Jaylin/Spider/tree/master/admin5\" target=\"_blank\" rel=\"noopener\">Github</a>， 具体实现请查看代码，注释已经写的比较清楚了。</p>\n<h4 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h4><div class=\"article-img\"><p><img src=\"/post/crawl-admin5/screenshot-5.png\" alt=\"截图-5\"></p></div>\n<p>已经实现了自动保存下载进度，可以随时<code>Ctrl+C</code>退出程序，下次执行时会跳过已下载的项目。</p>\n","prev":{"title":"远程溢出之古怪地鼠ERRATICGOPHER","slug":"exploit-erraticgopher"},"next":{"title":"昨天，明天","slug":"yesterday-and-tomorrow"},"link":"http://Jayl1n.github.io/post/crawl-admin5/","toc":[{"title":"项目介绍","id":"项目介绍","index":"1"},{"title":"分析页面","id":"分析页面","index":"2"},{"title":"运行流程","id":"运行流程","index":"3"},{"title":"具体实现","id":"具体实现","index":"4","children":[{"title":"模型","id":"模型","index":"4.1","children":[{"title":"代码","id":"代码","index":"4.1.1"},{"title":"效果","id":"效果","index":"4.1.2"}]}]}]}